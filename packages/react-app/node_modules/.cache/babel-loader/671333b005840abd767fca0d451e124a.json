{"ast":null,"code":"'use strict';\n\nconst {\n  DAGNode\n} = require('ipld-dag-pb');\n\nconst Bucket = require('hamt-sharding/src/bucket');\n\nconst DirSharded = require('ipfs-unixfs-importer/src/dir-sharded');\n\nconst log = require('debug')('ipfs:mfs:core:utils:hamt-utils');\n\nconst UnixFS = require('ipfs-unixfs');\n\nconst mc = require('multicodec');\n\nconst mh = require('multihashes');\n\nconst last = require('async-iterator-last');\n\nconst updateHamtDirectory = async (context, links, bucket, options) => {\n  // update parent with new bit field\n  const data = Buffer.from(bucket._children.bitField().reverse());\n  const dir = new UnixFS('hamt-sharded-directory', data);\n  dir.fanout = bucket.tableSize();\n  dir.hashType = DirSharded.hashFn.code;\n  const format = mc[options.format.toUpperCase().replace(/-/g, '_')];\n  const hashAlg = mh.names[options.hashAlg];\n  const parent = new DAGNode(dir.marshal(), links);\n  const cid = await context.ipld.put(parent, format, {\n    cidVersion: options.cidVersion,\n    hashAlg,\n    hashOnly: !options.flush\n  });\n  return {\n    node: parent,\n    cid\n  };\n};\n\nconst recreateHamtLevel = async (links, rootBucket, parentBucket, positionAtParent) => {\n  // recreate this level of the HAMT\n  const bucket = new Bucket({\n    hashFn: DirSharded.hashFn,\n    hash: parentBucket ? parentBucket._options.hash : undefined\n  }, parentBucket, positionAtParent);\n\n  if (parentBucket) {\n    parentBucket._putObjectAt(positionAtParent, bucket);\n  }\n\n  await addLinksToHamtBucket(links, bucket, rootBucket);\n  return bucket;\n};\n\nconst addLinksToHamtBucket = async (links, bucket, rootBucket) => {\n  await Promise.all(links.map(link => {\n    if (link.Name.length === 2) {\n      const pos = parseInt(link.Name, 16);\n\n      bucket._putObjectAt(pos, new Bucket({\n        hashFn: DirSharded.hashFn\n      }, bucket, pos));\n\n      return Promise.resolve();\n    }\n\n    return (rootBucket || bucket).put(link.Name.substring(2), {\n      size: link.TSize,\n      cid: link.Hash\n    });\n  }));\n};\n\nconst toPrefix = position => {\n  return position.toString('16').toUpperCase().padStart(2, '0').substring(0, 2);\n};\n\nconst generatePath = async (context, fileName, rootNode) => {\n  // start at the root bucket and descend, loading nodes as we go\n  const rootBucket = await recreateHamtLevel(rootNode.Links, null, null, null);\n  const position = await rootBucket._findNewBucketAndPos(fileName); // the path to the root bucket\n\n  const path = [{\n    bucket: position.bucket,\n    prefix: toPrefix(position.pos)\n  }];\n  let currentBucket = position.bucket;\n\n  while (currentBucket !== rootBucket) {\n    path.push({\n      bucket: currentBucket,\n      prefix: toPrefix(currentBucket._posAtParent)\n    });\n    currentBucket = currentBucket._parent;\n  }\n\n  path.reverse();\n  path[0].node = rootNode; // load DAGNode for each path segment\n\n  for (let i = 0; i < path.length; i++) {\n    const segment = path[i]; // find prefix in links\n\n    const link = segment.node.Links.filter(link => link.Name.substring(0, 2) === segment.prefix).pop(); // entry was not in shard\n\n    if (!link) {\n      // reached bottom of tree, file will be added to the current bucket\n      log(`Link ${segment.prefix}${fileName} will be added`); // return path\n\n      continue;\n    } // found entry\n\n\n    if (link.Name === `${segment.prefix}${fileName}`) {\n      log(`Link ${segment.prefix}${fileName} will be replaced`); // file already existed, file will be added to the current bucket\n      // return path\n\n      continue;\n    } // found subshard\n\n\n    log(`Found subshard ${segment.prefix}`);\n    const node = await context.ipld.get(link.Hash); // subshard hasn't been loaded, descend to the next level of the HAMT\n\n    if (!path[i + 1]) {\n      log(`Loaded new subshard ${segment.prefix}`);\n      await recreateHamtLevel(node.Links, rootBucket, segment.bucket, parseInt(segment.prefix, 16));\n      const position = await rootBucket._findNewBucketAndPos(fileName); // i--\n\n      path.push({\n        bucket: position.bucket,\n        prefix: toPrefix(position.pos),\n        node: node\n      });\n      continue;\n    }\n\n    const nextSegment = path[i + 1]; // add intermediate links to bucket\n\n    await addLinksToHamtBucket(node.Links, nextSegment.bucket, rootBucket);\n    nextSegment.node = node;\n  }\n\n  await rootBucket.put(fileName, true);\n  path.reverse();\n  return {\n    rootBucket,\n    path\n  };\n};\n\nconst createShard = async (context, contents, options) => {\n  const shard = new DirSharded({\n    root: true,\n    dir: true,\n    parent: null,\n    parentKey: null,\n    path: '',\n    dirty: true,\n    flat: false\n  }, options);\n\n  for (let i = 0; i < contents.length; i++) {\n    await shard._bucket.put(contents[i].name, {\n      size: contents[i].size,\n      cid: contents[i].cid\n    });\n  }\n\n  return last(shard.flush('', context.ipld, null));\n};\n\nmodule.exports = {\n  generatePath,\n  updateHamtDirectory,\n  recreateHamtLevel,\n  addLinksToHamtBucket,\n  toPrefix,\n  createShard\n};","map":{"version":3,"sources":["/home/dekan/Projects/raid-guild/dao-badges-web/node_modules/ipfs-mfs/src/core/utils/hamt-utils.js"],"names":["DAGNode","require","Bucket","DirSharded","log","UnixFS","mc","mh","last","updateHamtDirectory","context","links","bucket","options","data","Buffer","from","_children","bitField","reverse","dir","fanout","tableSize","hashType","hashFn","code","format","toUpperCase","replace","hashAlg","names","parent","marshal","cid","ipld","put","cidVersion","hashOnly","flush","node","recreateHamtLevel","rootBucket","parentBucket","positionAtParent","hash","_options","undefined","_putObjectAt","addLinksToHamtBucket","Promise","all","map","link","Name","length","pos","parseInt","resolve","substring","size","TSize","Hash","toPrefix","position","toString","padStart","generatePath","fileName","rootNode","Links","_findNewBucketAndPos","path","prefix","currentBucket","push","_posAtParent","_parent","i","segment","filter","pop","get","nextSegment","createShard","contents","shard","root","parentKey","dirty","flat","_bucket","name","module","exports"],"mappings":"AAAA;;AAEA,MAAM;AACJA,EAAAA;AADI,IAEFC,OAAO,CAAC,aAAD,CAFX;;AAGA,MAAMC,MAAM,GAAGD,OAAO,CAAC,0BAAD,CAAtB;;AACA,MAAME,UAAU,GAAGF,OAAO,CAAC,sCAAD,CAA1B;;AACA,MAAMG,GAAG,GAAGH,OAAO,CAAC,OAAD,CAAP,CAAiB,gCAAjB,CAAZ;;AACA,MAAMI,MAAM,GAAGJ,OAAO,CAAC,aAAD,CAAtB;;AACA,MAAMK,EAAE,GAAGL,OAAO,CAAC,YAAD,CAAlB;;AACA,MAAMM,EAAE,GAAGN,OAAO,CAAC,aAAD,CAAlB;;AACA,MAAMO,IAAI,GAAGP,OAAO,CAAC,qBAAD,CAApB;;AAEA,MAAMQ,mBAAmB,GAAG,OAAOC,OAAP,EAAgBC,KAAhB,EAAuBC,MAAvB,EAA+BC,OAA/B,KAA2C;AACrE;AACA,QAAMC,IAAI,GAAGC,MAAM,CAACC,IAAP,CAAYJ,MAAM,CAACK,SAAP,CAAiBC,QAAjB,GAA4BC,OAA5B,EAAZ,CAAb;AACA,QAAMC,GAAG,GAAG,IAAIf,MAAJ,CAAW,wBAAX,EAAqCS,IAArC,CAAZ;AACAM,EAAAA,GAAG,CAACC,MAAJ,GAAaT,MAAM,CAACU,SAAP,EAAb;AACAF,EAAAA,GAAG,CAACG,QAAJ,GAAepB,UAAU,CAACqB,MAAX,CAAkBC,IAAjC;AAEA,QAAMC,MAAM,GAAGpB,EAAE,CAACO,OAAO,CAACa,MAAR,CAAeC,WAAf,GAA6BC,OAA7B,CAAqC,IAArC,EAA2C,GAA3C,CAAD,CAAjB;AACA,QAAMC,OAAO,GAAGtB,EAAE,CAACuB,KAAH,CAASjB,OAAO,CAACgB,OAAjB,CAAhB;AAEA,QAAME,MAAM,GAAG,IAAI/B,OAAJ,CAAYoB,GAAG,CAACY,OAAJ,EAAZ,EAA2BrB,KAA3B,CAAf;AACA,QAAMsB,GAAG,GAAG,MAAMvB,OAAO,CAACwB,IAAR,CAAaC,GAAb,CAAiBJ,MAAjB,EAAyBL,MAAzB,EAAiC;AACjDU,IAAAA,UAAU,EAAEvB,OAAO,CAACuB,UAD6B;AAEjDP,IAAAA,OAFiD;AAGjDQ,IAAAA,QAAQ,EAAE,CAACxB,OAAO,CAACyB;AAH8B,GAAjC,CAAlB;AAMA,SAAO;AACLC,IAAAA,IAAI,EAAER,MADD;AAELE,IAAAA;AAFK,GAAP;AAID,CArBD;;AAuBA,MAAMO,iBAAiB,GAAG,OAAO7B,KAAP,EAAc8B,UAAd,EAA0BC,YAA1B,EAAwCC,gBAAxC,KAA6D;AACrF;AACA,QAAM/B,MAAM,GAAG,IAAIV,MAAJ,CAAW;AACxBsB,IAAAA,MAAM,EAAErB,UAAU,CAACqB,MADK;AAExBoB,IAAAA,IAAI,EAAEF,YAAY,GAAGA,YAAY,CAACG,QAAb,CAAsBD,IAAzB,GAAgCE;AAF1B,GAAX,EAGZJ,YAHY,EAGEC,gBAHF,CAAf;;AAKA,MAAID,YAAJ,EAAkB;AAChBA,IAAAA,YAAY,CAACK,YAAb,CAA0BJ,gBAA1B,EAA4C/B,MAA5C;AACD;;AAED,QAAMoC,oBAAoB,CAACrC,KAAD,EAAQC,MAAR,EAAgB6B,UAAhB,CAA1B;AAEA,SAAO7B,MAAP;AACD,CAdD;;AAgBA,MAAMoC,oBAAoB,GAAG,OAAOrC,KAAP,EAAcC,MAAd,EAAsB6B,UAAtB,KAAqC;AAChE,QAAMQ,OAAO,CAACC,GAAR,CACJvC,KAAK,CAACwC,GAAN,CAAUC,IAAI,IAAI;AAChB,QAAIA,IAAI,CAACC,IAAL,CAAUC,MAAV,KAAqB,CAAzB,EAA4B;AAC1B,YAAMC,GAAG,GAAGC,QAAQ,CAACJ,IAAI,CAACC,IAAN,EAAY,EAAZ,CAApB;;AAEAzC,MAAAA,MAAM,CAACmC,YAAP,CAAoBQ,GAApB,EAAyB,IAAIrD,MAAJ,CAAW;AAClCsB,QAAAA,MAAM,EAAErB,UAAU,CAACqB;AADe,OAAX,EAEtBZ,MAFsB,EAEd2C,GAFc,CAAzB;;AAIA,aAAON,OAAO,CAACQ,OAAR,EAAP;AACD;;AAED,WAAO,CAAChB,UAAU,IAAI7B,MAAf,EAAuBuB,GAAvB,CAA2BiB,IAAI,CAACC,IAAL,CAAUK,SAAV,CAAoB,CAApB,CAA3B,EAAmD;AACxDC,MAAAA,IAAI,EAAEP,IAAI,CAACQ,KAD6C;AAExD3B,MAAAA,GAAG,EAAEmB,IAAI,CAACS;AAF8C,KAAnD,CAAP;AAID,GAfD,CADI,CAAN;AAkBD,CAnBD;;AAqBA,MAAMC,QAAQ,GAAIC,QAAD,IAAc;AAC7B,SAAOA,QAAQ,CACZC,QADI,CACK,IADL,EAEJrC,WAFI,GAGJsC,QAHI,CAGK,CAHL,EAGQ,GAHR,EAIJP,SAJI,CAIM,CAJN,EAIS,CAJT,CAAP;AAKD,CAND;;AAQA,MAAMQ,YAAY,GAAG,OAAOxD,OAAP,EAAgByD,QAAhB,EAA0BC,QAA1B,KAAuC;AAC1D;AACA,QAAM3B,UAAU,GAAG,MAAMD,iBAAiB,CAAC4B,QAAQ,CAACC,KAAV,EAAiB,IAAjB,EAAuB,IAAvB,EAA6B,IAA7B,CAA1C;AACA,QAAMN,QAAQ,GAAG,MAAMtB,UAAU,CAAC6B,oBAAX,CAAgCH,QAAhC,CAAvB,CAH0D,CAK1D;;AACA,QAAMI,IAAI,GAAG,CAAC;AACZ3D,IAAAA,MAAM,EAAEmD,QAAQ,CAACnD,MADL;AAEZ4D,IAAAA,MAAM,EAAEV,QAAQ,CAACC,QAAQ,CAACR,GAAV;AAFJ,GAAD,CAAb;AAIA,MAAIkB,aAAa,GAAGV,QAAQ,CAACnD,MAA7B;;AAEA,SAAO6D,aAAa,KAAKhC,UAAzB,EAAqC;AACnC8B,IAAAA,IAAI,CAACG,IAAL,CAAU;AACR9D,MAAAA,MAAM,EAAE6D,aADA;AAERD,MAAAA,MAAM,EAAEV,QAAQ,CAACW,aAAa,CAACE,YAAf;AAFR,KAAV;AAKAF,IAAAA,aAAa,GAAGA,aAAa,CAACG,OAA9B;AACD;;AAEDL,EAAAA,IAAI,CAACpD,OAAL;AACAoD,EAAAA,IAAI,CAAC,CAAD,CAAJ,CAAQhC,IAAR,GAAe6B,QAAf,CAtB0D,CAwB1D;;AACA,OAAK,IAAIS,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAGN,IAAI,CAACjB,MAAzB,EAAiCuB,CAAC,EAAlC,EAAsC;AACpC,UAAMC,OAAO,GAAGP,IAAI,CAACM,CAAD,CAApB,CADoC,CAGpC;;AACA,UAAMzB,IAAI,GAAG0B,OAAO,CAACvC,IAAR,CAAa8B,KAAb,CACVU,MADU,CACH3B,IAAI,IAAIA,IAAI,CAACC,IAAL,CAAUK,SAAV,CAAoB,CAApB,EAAuB,CAAvB,MAA8BoB,OAAO,CAACN,MAD3C,EAEVQ,GAFU,EAAb,CAJoC,CAQpC;;AACA,QAAI,CAAC5B,IAAL,EAAW;AACT;AACAhD,MAAAA,GAAG,CAAE,QAAO0E,OAAO,CAACN,MAAO,GAAEL,QAAS,gBAAnC,CAAH,CAFS,CAGT;;AACA;AACD,KAdmC,CAgBpC;;;AACA,QAAIf,IAAI,CAACC,IAAL,KAAe,GAAEyB,OAAO,CAACN,MAAO,GAAEL,QAAS,EAA/C,EAAkD;AAChD/D,MAAAA,GAAG,CAAE,QAAO0E,OAAO,CAACN,MAAO,GAAEL,QAAS,mBAAnC,CAAH,CADgD,CAEhD;AACA;;AACA;AACD,KAtBmC,CAwBpC;;;AACA/D,IAAAA,GAAG,CAAE,kBAAiB0E,OAAO,CAACN,MAAO,EAAlC,CAAH;AACA,UAAMjC,IAAI,GAAG,MAAM7B,OAAO,CAACwB,IAAR,CAAa+C,GAAb,CAAiB7B,IAAI,CAACS,IAAtB,CAAnB,CA1BoC,CA4BpC;;AACA,QAAI,CAACU,IAAI,CAACM,CAAC,GAAG,CAAL,CAAT,EAAkB;AAChBzE,MAAAA,GAAG,CAAE,uBAAsB0E,OAAO,CAACN,MAAO,EAAvC,CAAH;AAEA,YAAMhC,iBAAiB,CAACD,IAAI,CAAC8B,KAAN,EAAa5B,UAAb,EAAyBqC,OAAO,CAAClE,MAAjC,EAAyC4C,QAAQ,CAACsB,OAAO,CAACN,MAAT,EAAiB,EAAjB,CAAjD,CAAvB;AACA,YAAMT,QAAQ,GAAG,MAAMtB,UAAU,CAAC6B,oBAAX,CAAgCH,QAAhC,CAAvB,CAJgB,CAMhB;;AACAI,MAAAA,IAAI,CAACG,IAAL,CAAU;AACR9D,QAAAA,MAAM,EAAEmD,QAAQ,CAACnD,MADT;AAER4D,QAAAA,MAAM,EAAEV,QAAQ,CAACC,QAAQ,CAACR,GAAV,CAFR;AAGRhB,QAAAA,IAAI,EAAEA;AAHE,OAAV;AAMA;AACD;;AAED,UAAM2C,WAAW,GAAGX,IAAI,CAACM,CAAC,GAAG,CAAL,CAAxB,CA7CoC,CA+CpC;;AACA,UAAM7B,oBAAoB,CAACT,IAAI,CAAC8B,KAAN,EAAaa,WAAW,CAACtE,MAAzB,EAAiC6B,UAAjC,CAA1B;AAEAyC,IAAAA,WAAW,CAAC3C,IAAZ,GAAmBA,IAAnB;AACD;;AAED,QAAME,UAAU,CAACN,GAAX,CAAegC,QAAf,EAAyB,IAAzB,CAAN;AAEAI,EAAAA,IAAI,CAACpD,OAAL;AAEA,SAAO;AACLsB,IAAAA,UADK;AAEL8B,IAAAA;AAFK,GAAP;AAID,CAtFD;;AAwFA,MAAMY,WAAW,GAAG,OAAOzE,OAAP,EAAgB0E,QAAhB,EAA0BvE,OAA1B,KAAsC;AACxD,QAAMwE,KAAK,GAAG,IAAIlF,UAAJ,CAAe;AAC3BmF,IAAAA,IAAI,EAAE,IADqB;AAE3BlE,IAAAA,GAAG,EAAE,IAFsB;AAG3BW,IAAAA,MAAM,EAAE,IAHmB;AAI3BwD,IAAAA,SAAS,EAAE,IAJgB;AAK3BhB,IAAAA,IAAI,EAAE,EALqB;AAM3BiB,IAAAA,KAAK,EAAE,IANoB;AAO3BC,IAAAA,IAAI,EAAE;AAPqB,GAAf,EAQX5E,OARW,CAAd;;AAUA,OAAK,IAAIgE,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAGO,QAAQ,CAAC9B,MAA7B,EAAqCuB,CAAC,EAAtC,EAA0C;AACxC,UAAMQ,KAAK,CAACK,OAAN,CAAcvD,GAAd,CAAkBiD,QAAQ,CAACP,CAAD,CAAR,CAAYc,IAA9B,EAAoC;AACxChC,MAAAA,IAAI,EAAEyB,QAAQ,CAACP,CAAD,CAAR,CAAYlB,IADsB;AAExC1B,MAAAA,GAAG,EAAEmD,QAAQ,CAACP,CAAD,CAAR,CAAY5C;AAFuB,KAApC,CAAN;AAID;;AAED,SAAOzB,IAAI,CAAC6E,KAAK,CAAC/C,KAAN,CAAY,EAAZ,EAAgB5B,OAAO,CAACwB,IAAxB,EAA8B,IAA9B,CAAD,CAAX;AACD,CAnBD;;AAqBA0D,MAAM,CAACC,OAAP,GAAiB;AACf3B,EAAAA,YADe;AAEfzD,EAAAA,mBAFe;AAGf+B,EAAAA,iBAHe;AAIfQ,EAAAA,oBAJe;AAKfc,EAAAA,QALe;AAMfqB,EAAAA;AANe,CAAjB","sourcesContent":["'use strict'\n\nconst {\n  DAGNode\n} = require('ipld-dag-pb')\nconst Bucket = require('hamt-sharding/src/bucket')\nconst DirSharded = require('ipfs-unixfs-importer/src/dir-sharded')\nconst log = require('debug')('ipfs:mfs:core:utils:hamt-utils')\nconst UnixFS = require('ipfs-unixfs')\nconst mc = require('multicodec')\nconst mh = require('multihashes')\nconst last = require('async-iterator-last')\n\nconst updateHamtDirectory = async (context, links, bucket, options) => {\n  // update parent with new bit field\n  const data = Buffer.from(bucket._children.bitField().reverse())\n  const dir = new UnixFS('hamt-sharded-directory', data)\n  dir.fanout = bucket.tableSize()\n  dir.hashType = DirSharded.hashFn.code\n\n  const format = mc[options.format.toUpperCase().replace(/-/g, '_')]\n  const hashAlg = mh.names[options.hashAlg]\n\n  const parent = new DAGNode(dir.marshal(), links)\n  const cid = await context.ipld.put(parent, format, {\n    cidVersion: options.cidVersion,\n    hashAlg,\n    hashOnly: !options.flush\n  })\n\n  return {\n    node: parent,\n    cid\n  }\n}\n\nconst recreateHamtLevel = async (links, rootBucket, parentBucket, positionAtParent) => {\n  // recreate this level of the HAMT\n  const bucket = new Bucket({\n    hashFn: DirSharded.hashFn,\n    hash: parentBucket ? parentBucket._options.hash : undefined\n  }, parentBucket, positionAtParent)\n\n  if (parentBucket) {\n    parentBucket._putObjectAt(positionAtParent, bucket)\n  }\n\n  await addLinksToHamtBucket(links, bucket, rootBucket)\n\n  return bucket\n}\n\nconst addLinksToHamtBucket = async (links, bucket, rootBucket) => {\n  await Promise.all(\n    links.map(link => {\n      if (link.Name.length === 2) {\n        const pos = parseInt(link.Name, 16)\n\n        bucket._putObjectAt(pos, new Bucket({\n          hashFn: DirSharded.hashFn\n        }, bucket, pos))\n\n        return Promise.resolve()\n      }\n\n      return (rootBucket || bucket).put(link.Name.substring(2), {\n        size: link.TSize,\n        cid: link.Hash\n      })\n    })\n  )\n}\n\nconst toPrefix = (position) => {\n  return position\n    .toString('16')\n    .toUpperCase()\n    .padStart(2, '0')\n    .substring(0, 2)\n}\n\nconst generatePath = async (context, fileName, rootNode) => {\n  // start at the root bucket and descend, loading nodes as we go\n  const rootBucket = await recreateHamtLevel(rootNode.Links, null, null, null)\n  const position = await rootBucket._findNewBucketAndPos(fileName)\n\n  // the path to the root bucket\n  const path = [{\n    bucket: position.bucket,\n    prefix: toPrefix(position.pos)\n  }]\n  let currentBucket = position.bucket\n\n  while (currentBucket !== rootBucket) {\n    path.push({\n      bucket: currentBucket,\n      prefix: toPrefix(currentBucket._posAtParent)\n    })\n\n    currentBucket = currentBucket._parent\n  }\n\n  path.reverse()\n  path[0].node = rootNode\n\n  // load DAGNode for each path segment\n  for (let i = 0; i < path.length; i++) {\n    const segment = path[i]\n\n    // find prefix in links\n    const link = segment.node.Links\n      .filter(link => link.Name.substring(0, 2) === segment.prefix)\n      .pop()\n\n    // entry was not in shard\n    if (!link) {\n      // reached bottom of tree, file will be added to the current bucket\n      log(`Link ${segment.prefix}${fileName} will be added`)\n      // return path\n      continue\n    }\n\n    // found entry\n    if (link.Name === `${segment.prefix}${fileName}`) {\n      log(`Link ${segment.prefix}${fileName} will be replaced`)\n      // file already existed, file will be added to the current bucket\n      // return path\n      continue\n    }\n\n    // found subshard\n    log(`Found subshard ${segment.prefix}`)\n    const node = await context.ipld.get(link.Hash)\n\n    // subshard hasn't been loaded, descend to the next level of the HAMT\n    if (!path[i + 1]) {\n      log(`Loaded new subshard ${segment.prefix}`)\n\n      await recreateHamtLevel(node.Links, rootBucket, segment.bucket, parseInt(segment.prefix, 16))\n      const position = await rootBucket._findNewBucketAndPos(fileName)\n\n      // i--\n      path.push({\n        bucket: position.bucket,\n        prefix: toPrefix(position.pos),\n        node: node\n      })\n\n      continue\n    }\n\n    const nextSegment = path[i + 1]\n\n    // add intermediate links to bucket\n    await addLinksToHamtBucket(node.Links, nextSegment.bucket, rootBucket)\n\n    nextSegment.node = node\n  }\n\n  await rootBucket.put(fileName, true)\n\n  path.reverse()\n\n  return {\n    rootBucket,\n    path\n  }\n}\n\nconst createShard = async (context, contents, options) => {\n  const shard = new DirSharded({\n    root: true,\n    dir: true,\n    parent: null,\n    parentKey: null,\n    path: '',\n    dirty: true,\n    flat: false\n  }, options)\n\n  for (let i = 0; i < contents.length; i++) {\n    await shard._bucket.put(contents[i].name, {\n      size: contents[i].size,\n      cid: contents[i].cid\n    })\n  }\n\n  return last(shard.flush('', context.ipld, null))\n}\n\nmodule.exports = {\n  generatePath,\n  updateHamtDirectory,\n  recreateHamtLevel,\n  addLinksToHamtBucket,\n  toPrefix,\n  createShard\n}\n"]},"metadata":{},"sourceType":"script"}