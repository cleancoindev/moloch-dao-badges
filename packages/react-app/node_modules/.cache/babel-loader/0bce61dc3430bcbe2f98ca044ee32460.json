{"ast":null,"code":"'use strict';\n\nconst {\n  DAGLink\n} = require('ipld-dag-pb');\n\nconst CID = require('cids');\n\nconst log = require('debug')('ipfs:mfs:core:utils:add-link');\n\nconst UnixFS = require('ipfs-unixfs');\n\nconst DirSharded = require('ipfs-unixfs-importer/src/dir-sharded');\n\nconst {\n  updateHamtDirectory,\n  recreateHamtLevel,\n  createShard,\n  toPrefix,\n  addLinksToHamtBucket\n} = require('./hamt-utils');\n\nconst errCode = require('err-code');\n\nconst mc = require('multicodec');\n\nconst mh = require('multihashes');\n\nconst last = require('async-iterator-last');\n\nconst addLink = async (context, options) => {\n  if (!options.parentCid && !options.parent) {\n    throw errCode(new Error('No parent node or CID passed to addLink'), 'EINVALIDPARENT');\n  }\n\n  if (options.parentCid && !CID.isCID(options.parentCid)) {\n    throw errCode(new Error('Invalid CID passed to addLink'), 'EINVALIDPARENTCID');\n  }\n\n  if (!options.parent) {\n    log(`Loading parent node ${options.parentCid}`);\n    options.parent = await context.ipld.get(options.parentCid);\n  }\n\n  if (!options.cid) {\n    throw errCode(new Error('No child cid passed to addLink'), 'EINVALIDCHILDCID');\n  }\n\n  if (!options.name) {\n    throw errCode(new Error('No child name passed to addLink'), 'EINVALIDCHILDNAME');\n  }\n\n  if (!CID.isCID(options.cid)) {\n    options.cid = new CID(options.cid);\n  }\n\n  if (!options.size && options.size !== 0) {\n    throw errCode(new Error('No child size passed to addLink'), 'EINVALIDCHILDSIZE');\n  }\n\n  const meta = UnixFS.unmarshal(options.parent.Data);\n\n  if (meta.type === 'hamt-sharded-directory') {\n    log('Adding link to sharded directory');\n    return addToShardedDirectory(context, options);\n  }\n\n  if (options.parent.Links.length >= options.shardSplitThreshold) {\n    log('Converting directory to sharded directory');\n    return convertToShardedDirectory(context, options);\n  }\n\n  log(`Adding ${options.name} (${options.cid}) to regular directory`);\n  return addToDirectory(context, options);\n};\n\nconst convertToShardedDirectory = async (context, options) => {\n  const result = await createShard(context, options.parent.Links.map(link => ({\n    name: link.Name,\n    size: link.Tsize,\n    cid: link.Hash\n  })).concat({\n    name: options.name,\n    size: options.size,\n    cid: options.cid\n  }), options);\n  log(`Converted directory to sharded directory ${result.cid}`);\n  return result;\n};\n\nconst addToDirectory = async (context, options) => {\n  options.parent.rmLink(options.name);\n  options.parent.addLink(new DAGLink(options.name, options.size, options.cid));\n  const format = mc[options.format.toUpperCase().replace(/-/g, '_')];\n  const hashAlg = mh.names[options.hashAlg]; // Persist the new parent DAGNode\n\n  const cid = await context.ipld.put(options.parent, format, {\n    cidVersion: options.cidVersion,\n    hashAlg,\n    hashOnly: !options.flush\n  });\n  return {\n    node: options.parent,\n    cid\n  };\n};\n\nconst addToShardedDirectory = async (context, options) => {\n  const {\n    shard,\n    path\n  } = await addFileToShardedDirectory(context, options);\n  const result = await last(shard.flush('', context.ipld)); // we have written out the shard, but only one sub-shard will have been written so replace it in the original shard\n\n  const oldLink = options.parent.Links.find(link => link.Name.substring(0, 2) === path[0].prefix);\n  const newLink = result.node.Links.find(link => link.Name.substring(0, 2) === path[0].prefix);\n\n  if (oldLink) {\n    options.parent.rmLink(oldLink.Name);\n  }\n\n  options.parent.addLink(newLink);\n  return updateHamtDirectory(context, options.parent.Links, path[0].bucket, options);\n};\n\nconst addFileToShardedDirectory = async (context, options) => {\n  const file = {\n    name: options.name,\n    cid: options.cid,\n    size: options.size\n  }; // start at the root bucket and descend, loading nodes as we go\n\n  const rootBucket = await recreateHamtLevel(options.parent.Links);\n  const shard = new DirSharded({\n    root: true,\n    dir: true,\n    parent: null,\n    parentKey: null,\n    path: '',\n    dirty: true,\n    flat: false\n  }, options);\n  shard._bucket = rootBucket; // load subshards until the bucket & position no longer changes\n\n  const position = await rootBucket._findNewBucketAndPos(file.name);\n  const path = toBucketPath(position);\n  path[0].node = options.parent;\n  let index = 0;\n\n  while (index < path.length) {\n    const segment = path[index];\n    index++;\n    const node = segment.node;\n    const link = node.Links.find(link => link.Name.substring(0, 2) === segment.prefix);\n\n    if (!link) {\n      // prefix is new, file will be added to the current bucket\n      log(`Link ${segment.prefix}${file.name} will be added`);\n      index = path.length;\n      break;\n    }\n\n    if (link.Name === `${segment.prefix}${file.name}`) {\n      // file already existed, file will be added to the current bucket\n      log(`Link ${segment.prefix}${file.name} will be replaced`);\n      index = path.length;\n      break;\n    }\n\n    if (link.Name.length > 2) {\n      // another file had the same prefix, will be replaced with a subshard\n      log(`Link ${link.Name} will be replaced with a subshard`);\n      index = path.length;\n      break;\n    } // load sub-shard\n\n\n    log(`Found subshard ${segment.prefix}`);\n    const subShard = await context.ipld.get(link.Hash); // subshard hasn't been loaded, descend to the next level of the HAMT\n\n    if (!path[index]) {\n      log(`Loaded new subshard ${segment.prefix}`);\n      await recreateHamtLevel(subShard.Links, rootBucket, segment.bucket, parseInt(segment.prefix, 16));\n      const position = await rootBucket._findNewBucketAndPos(file.name);\n      path.push({\n        bucket: position.bucket,\n        prefix: toPrefix(position.pos),\n        node: subShard\n      });\n      break;\n    }\n\n    const nextSegment = path[index]; // add next level's worth of links to bucket\n\n    await addLinksToHamtBucket(subShard.Links, nextSegment.bucket, rootBucket);\n    nextSegment.node = subShard;\n  } // finally add the new file into the shard\n\n\n  await shard._bucket.put(file.name, {\n    size: file.size,\n    cid: file.cid\n  });\n  return {\n    shard,\n    path\n  };\n};\n\nconst toBucketPath = position => {\n  let bucket = position.bucket;\n  let positionInBucket = position.pos;\n  const path = [{\n    bucket,\n    prefix: toPrefix(positionInBucket)\n  }];\n  bucket = position.bucket._parent;\n  positionInBucket = position.bucket._posAtParent;\n\n  while (bucket) {\n    path.push({\n      bucket,\n      prefix: toPrefix(positionInBucket)\n    });\n    positionInBucket = bucket._posAtParent;\n    bucket = bucket._parent;\n  }\n\n  path.reverse();\n  return path;\n};\n\nmodule.exports = addLink;","map":{"version":3,"sources":["/home/dekan/Projects/raid-guild/dao-badges-web/node_modules/ipfs-mfs/src/core/utils/add-link.js"],"names":["DAGLink","require","CID","log","UnixFS","DirSharded","updateHamtDirectory","recreateHamtLevel","createShard","toPrefix","addLinksToHamtBucket","errCode","mc","mh","last","addLink","context","options","parentCid","parent","Error","isCID","ipld","get","cid","name","size","meta","unmarshal","Data","type","addToShardedDirectory","Links","length","shardSplitThreshold","convertToShardedDirectory","addToDirectory","result","map","link","Name","Tsize","Hash","concat","rmLink","format","toUpperCase","replace","hashAlg","names","put","cidVersion","hashOnly","flush","node","shard","path","addFileToShardedDirectory","oldLink","find","substring","prefix","newLink","bucket","file","rootBucket","root","dir","parentKey","dirty","flat","_bucket","position","_findNewBucketAndPos","toBucketPath","index","segment","subShard","parseInt","push","pos","nextSegment","positionInBucket","_parent","_posAtParent","reverse","module","exports"],"mappings":"AAAA;;AAEA,MAAM;AACJA,EAAAA;AADI,IAEFC,OAAO,CAAC,aAAD,CAFX;;AAGA,MAAMC,GAAG,GAAGD,OAAO,CAAC,MAAD,CAAnB;;AACA,MAAME,GAAG,GAAGF,OAAO,CAAC,OAAD,CAAP,CAAiB,8BAAjB,CAAZ;;AACA,MAAMG,MAAM,GAAGH,OAAO,CAAC,aAAD,CAAtB;;AACA,MAAMI,UAAU,GAAGJ,OAAO,CAAC,sCAAD,CAA1B;;AACA,MAAM;AACJK,EAAAA,mBADI;AAEJC,EAAAA,iBAFI;AAGJC,EAAAA,WAHI;AAIJC,EAAAA,QAJI;AAKJC,EAAAA;AALI,IAMFT,OAAO,CAAC,cAAD,CANX;;AAOA,MAAMU,OAAO,GAAGV,OAAO,CAAC,UAAD,CAAvB;;AACA,MAAMW,EAAE,GAAGX,OAAO,CAAC,YAAD,CAAlB;;AACA,MAAMY,EAAE,GAAGZ,OAAO,CAAC,aAAD,CAAlB;;AACA,MAAMa,IAAI,GAAGb,OAAO,CAAC,qBAAD,CAApB;;AAEA,MAAMc,OAAO,GAAG,OAAOC,OAAP,EAAgBC,OAAhB,KAA4B;AAC1C,MAAI,CAACA,OAAO,CAACC,SAAT,IAAsB,CAACD,OAAO,CAACE,MAAnC,EAA2C;AACzC,UAAMR,OAAO,CAAC,IAAIS,KAAJ,CAAU,yCAAV,CAAD,EAAuD,gBAAvD,CAAb;AACD;;AAED,MAAIH,OAAO,CAACC,SAAR,IAAqB,CAAChB,GAAG,CAACmB,KAAJ,CAAUJ,OAAO,CAACC,SAAlB,CAA1B,EAAwD;AACtD,UAAMP,OAAO,CAAC,IAAIS,KAAJ,CAAU,+BAAV,CAAD,EAA6C,mBAA7C,CAAb;AACD;;AAED,MAAI,CAACH,OAAO,CAACE,MAAb,EAAqB;AACnBhB,IAAAA,GAAG,CAAE,uBAAsBc,OAAO,CAACC,SAAU,EAA1C,CAAH;AAEAD,IAAAA,OAAO,CAACE,MAAR,GAAiB,MAAMH,OAAO,CAACM,IAAR,CAAaC,GAAb,CAAiBN,OAAO,CAACC,SAAzB,CAAvB;AACD;;AAED,MAAI,CAACD,OAAO,CAACO,GAAb,EAAkB;AAChB,UAAMb,OAAO,CAAC,IAAIS,KAAJ,CAAU,gCAAV,CAAD,EAA8C,kBAA9C,CAAb;AACD;;AAED,MAAI,CAACH,OAAO,CAACQ,IAAb,EAAmB;AACjB,UAAMd,OAAO,CAAC,IAAIS,KAAJ,CAAU,iCAAV,CAAD,EAA+C,mBAA/C,CAAb;AACD;;AAED,MAAI,CAAClB,GAAG,CAACmB,KAAJ,CAAUJ,OAAO,CAACO,GAAlB,CAAL,EAA6B;AAC3BP,IAAAA,OAAO,CAACO,GAAR,GAAc,IAAItB,GAAJ,CAAQe,OAAO,CAACO,GAAhB,CAAd;AACD;;AAED,MAAI,CAACP,OAAO,CAACS,IAAT,IAAiBT,OAAO,CAACS,IAAR,KAAiB,CAAtC,EAAyC;AACvC,UAAMf,OAAO,CAAC,IAAIS,KAAJ,CAAU,iCAAV,CAAD,EAA+C,mBAA/C,CAAb;AACD;;AAED,QAAMO,IAAI,GAAGvB,MAAM,CAACwB,SAAP,CAAiBX,OAAO,CAACE,MAAR,CAAeU,IAAhC,CAAb;;AAEA,MAAIF,IAAI,CAACG,IAAL,KAAc,wBAAlB,EAA4C;AAC1C3B,IAAAA,GAAG,CAAC,kCAAD,CAAH;AAEA,WAAO4B,qBAAqB,CAACf,OAAD,EAAUC,OAAV,CAA5B;AACD;;AAED,MAAIA,OAAO,CAACE,MAAR,CAAea,KAAf,CAAqBC,MAArB,IAA+BhB,OAAO,CAACiB,mBAA3C,EAAgE;AAC9D/B,IAAAA,GAAG,CAAC,2CAAD,CAAH;AAEA,WAAOgC,yBAAyB,CAACnB,OAAD,EAAUC,OAAV,CAAhC;AACD;;AAEDd,EAAAA,GAAG,CAAE,UAASc,OAAO,CAACQ,IAAK,KAAIR,OAAO,CAACO,GAAI,wBAAxC,CAAH;AAEA,SAAOY,cAAc,CAACpB,OAAD,EAAUC,OAAV,CAArB;AACD,CAhDD;;AAkDA,MAAMkB,yBAAyB,GAAG,OAAOnB,OAAP,EAAgBC,OAAhB,KAA4B;AAC5D,QAAMoB,MAAM,GAAG,MAAM7B,WAAW,CAACQ,OAAD,EAAUC,OAAO,CAACE,MAAR,CAAea,KAAf,CAAqBM,GAArB,CAAyBC,IAAI,KAAK;AAC1Ed,IAAAA,IAAI,EAAEc,IAAI,CAACC,IAD+D;AAE1Ed,IAAAA,IAAI,EAAEa,IAAI,CAACE,KAF+D;AAG1EjB,IAAAA,GAAG,EAAEe,IAAI,CAACG;AAHgE,GAAL,CAA7B,EAItCC,MAJsC,CAI/B;AACTlB,IAAAA,IAAI,EAAER,OAAO,CAACQ,IADL;AAETC,IAAAA,IAAI,EAAET,OAAO,CAACS,IAFL;AAGTF,IAAAA,GAAG,EAAEP,OAAO,CAACO;AAHJ,GAJ+B,CAAV,EAQ5BP,OAR4B,CAAhC;AAUAd,EAAAA,GAAG,CAAE,4CAA2CkC,MAAM,CAACb,GAAI,EAAxD,CAAH;AAEA,SAAOa,MAAP;AACD,CAdD;;AAgBA,MAAMD,cAAc,GAAG,OAAOpB,OAAP,EAAgBC,OAAhB,KAA4B;AACjDA,EAAAA,OAAO,CAACE,MAAR,CAAeyB,MAAf,CAAsB3B,OAAO,CAACQ,IAA9B;AACAR,EAAAA,OAAO,CAACE,MAAR,CAAeJ,OAAf,CAAuB,IAAIf,OAAJ,CAAYiB,OAAO,CAACQ,IAApB,EAA0BR,OAAO,CAACS,IAAlC,EAAwCT,OAAO,CAACO,GAAhD,CAAvB;AAEA,QAAMqB,MAAM,GAAGjC,EAAE,CAACK,OAAO,CAAC4B,MAAR,CAAeC,WAAf,GAA6BC,OAA7B,CAAqC,IAArC,EAA2C,GAA3C,CAAD,CAAjB;AACA,QAAMC,OAAO,GAAGnC,EAAE,CAACoC,KAAH,CAAShC,OAAO,CAAC+B,OAAjB,CAAhB,CALiD,CAOjD;;AACA,QAAMxB,GAAG,GAAG,MAAMR,OAAO,CAACM,IAAR,CAAa4B,GAAb,CAAiBjC,OAAO,CAACE,MAAzB,EAAiC0B,MAAjC,EAAyC;AACzDM,IAAAA,UAAU,EAAElC,OAAO,CAACkC,UADqC;AAEzDH,IAAAA,OAFyD;AAGzDI,IAAAA,QAAQ,EAAE,CAACnC,OAAO,CAACoC;AAHsC,GAAzC,CAAlB;AAMA,SAAO;AACLC,IAAAA,IAAI,EAAErC,OAAO,CAACE,MADT;AAELK,IAAAA;AAFK,GAAP;AAID,CAlBD;;AAoBA,MAAMO,qBAAqB,GAAG,OAAOf,OAAP,EAAgBC,OAAhB,KAA4B;AACxD,QAAM;AACJsC,IAAAA,KADI;AACGC,IAAAA;AADH,MAEF,MAAMC,yBAAyB,CAACzC,OAAD,EAAUC,OAAV,CAFnC;AAIA,QAAMoB,MAAM,GAAG,MAAMvB,IAAI,CAACyC,KAAK,CAACF,KAAN,CAAY,EAAZ,EAAgBrC,OAAO,CAACM,IAAxB,CAAD,CAAzB,CALwD,CAOxD;;AACA,QAAMoC,OAAO,GAAGzC,OAAO,CAACE,MAAR,CAAea,KAAf,CACb2B,IADa,CACRpB,IAAI,IAAIA,IAAI,CAACC,IAAL,CAAUoB,SAAV,CAAoB,CAApB,EAAuB,CAAvB,MAA8BJ,IAAI,CAAC,CAAD,CAAJ,CAAQK,MADtC,CAAhB;AAGA,QAAMC,OAAO,GAAGzB,MAAM,CAACiB,IAAP,CAAYtB,KAAZ,CACb2B,IADa,CACRpB,IAAI,IAAIA,IAAI,CAACC,IAAL,CAAUoB,SAAV,CAAoB,CAApB,EAAuB,CAAvB,MAA8BJ,IAAI,CAAC,CAAD,CAAJ,CAAQK,MADtC,CAAhB;;AAGA,MAAIH,OAAJ,EAAa;AACXzC,IAAAA,OAAO,CAACE,MAAR,CAAeyB,MAAf,CAAsBc,OAAO,CAAClB,IAA9B;AACD;;AAEDvB,EAAAA,OAAO,CAACE,MAAR,CAAeJ,OAAf,CAAuB+C,OAAvB;AAEA,SAAOxD,mBAAmB,CAACU,OAAD,EAAUC,OAAO,CAACE,MAAR,CAAea,KAAzB,EAAgCwB,IAAI,CAAC,CAAD,CAAJ,CAAQO,MAAxC,EAAgD9C,OAAhD,CAA1B;AACD,CArBD;;AAuBA,MAAMwC,yBAAyB,GAAG,OAAOzC,OAAP,EAAgBC,OAAhB,KAA4B;AAC5D,QAAM+C,IAAI,GAAG;AACXvC,IAAAA,IAAI,EAAER,OAAO,CAACQ,IADH;AAEXD,IAAAA,GAAG,EAAEP,OAAO,CAACO,GAFF;AAGXE,IAAAA,IAAI,EAAET,OAAO,CAACS;AAHH,GAAb,CAD4D,CAO5D;;AACA,QAAMuC,UAAU,GAAG,MAAM1D,iBAAiB,CAACU,OAAO,CAACE,MAAR,CAAea,KAAhB,CAA1C;AAEA,QAAMuB,KAAK,GAAG,IAAIlD,UAAJ,CAAe;AAC3B6D,IAAAA,IAAI,EAAE,IADqB;AAE3BC,IAAAA,GAAG,EAAE,IAFsB;AAG3BhD,IAAAA,MAAM,EAAE,IAHmB;AAI3BiD,IAAAA,SAAS,EAAE,IAJgB;AAK3BZ,IAAAA,IAAI,EAAE,EALqB;AAM3Ba,IAAAA,KAAK,EAAE,IANoB;AAO3BC,IAAAA,IAAI,EAAE;AAPqB,GAAf,EAQXrD,OARW,CAAd;AASAsC,EAAAA,KAAK,CAACgB,OAAN,GAAgBN,UAAhB,CAnB4D,CAqB5D;;AACA,QAAMO,QAAQ,GAAG,MAAMP,UAAU,CAACQ,oBAAX,CAAgCT,IAAI,CAACvC,IAArC,CAAvB;AACA,QAAM+B,IAAI,GAAGkB,YAAY,CAACF,QAAD,CAAzB;AACAhB,EAAAA,IAAI,CAAC,CAAD,CAAJ,CAAQF,IAAR,GAAerC,OAAO,CAACE,MAAvB;AACA,MAAIwD,KAAK,GAAG,CAAZ;;AAEA,SAAOA,KAAK,GAAGnB,IAAI,CAACvB,MAApB,EAA4B;AAC1B,UAAM2C,OAAO,GAAGpB,IAAI,CAACmB,KAAD,CAApB;AACAA,IAAAA,KAAK;AACL,UAAMrB,IAAI,GAAGsB,OAAO,CAACtB,IAArB;AAEA,UAAMf,IAAI,GAAGe,IAAI,CAACtB,KAAL,CACV2B,IADU,CACLpB,IAAI,IAAIA,IAAI,CAACC,IAAL,CAAUoB,SAAV,CAAoB,CAApB,EAAuB,CAAvB,MAA8BgB,OAAO,CAACf,MADzC,CAAb;;AAGA,QAAI,CAACtB,IAAL,EAAW;AACT;AACApC,MAAAA,GAAG,CAAE,QAAOyE,OAAO,CAACf,MAAO,GAAEG,IAAI,CAACvC,IAAK,gBAApC,CAAH;AACAkD,MAAAA,KAAK,GAAGnB,IAAI,CAACvB,MAAb;AAEA;AACD;;AAED,QAAIM,IAAI,CAACC,IAAL,KAAe,GAAEoC,OAAO,CAACf,MAAO,GAAEG,IAAI,CAACvC,IAAK,EAAhD,EAAmD;AACjD;AACAtB,MAAAA,GAAG,CAAE,QAAOyE,OAAO,CAACf,MAAO,GAAEG,IAAI,CAACvC,IAAK,mBAApC,CAAH;AACAkD,MAAAA,KAAK,GAAGnB,IAAI,CAACvB,MAAb;AAEA;AACD;;AAED,QAAIM,IAAI,CAACC,IAAL,CAAUP,MAAV,GAAmB,CAAvB,EAA0B;AACxB;AACA9B,MAAAA,GAAG,CAAE,QAAOoC,IAAI,CAACC,IAAK,mCAAnB,CAAH;AACAmC,MAAAA,KAAK,GAAGnB,IAAI,CAACvB,MAAb;AAEA;AACD,KA9ByB,CAgC1B;;;AACA9B,IAAAA,GAAG,CAAE,kBAAiByE,OAAO,CAACf,MAAO,EAAlC,CAAH;AACA,UAAMgB,QAAQ,GAAG,MAAM7D,OAAO,CAACM,IAAR,CAAaC,GAAb,CAAiBgB,IAAI,CAACG,IAAtB,CAAvB,CAlC0B,CAoC1B;;AACA,QAAI,CAACc,IAAI,CAACmB,KAAD,CAAT,EAAkB;AAChBxE,MAAAA,GAAG,CAAE,uBAAsByE,OAAO,CAACf,MAAO,EAAvC,CAAH;AACA,YAAMtD,iBAAiB,CAACsE,QAAQ,CAAC7C,KAAV,EAAiBiC,UAAjB,EAA6BW,OAAO,CAACb,MAArC,EAA6Ce,QAAQ,CAACF,OAAO,CAACf,MAAT,EAAiB,EAAjB,CAArD,CAAvB;AAEA,YAAMW,QAAQ,GAAG,MAAMP,UAAU,CAACQ,oBAAX,CAAgCT,IAAI,CAACvC,IAArC,CAAvB;AAEA+B,MAAAA,IAAI,CAACuB,IAAL,CAAU;AACRhB,QAAAA,MAAM,EAAES,QAAQ,CAACT,MADT;AAERF,QAAAA,MAAM,EAAEpD,QAAQ,CAAC+D,QAAQ,CAACQ,GAAV,CAFR;AAGR1B,QAAAA,IAAI,EAAEuB;AAHE,OAAV;AAMA;AACD;;AAED,UAAMI,WAAW,GAAGzB,IAAI,CAACmB,KAAD,CAAxB,CApD0B,CAsD1B;;AACA,UAAMjE,oBAAoB,CAACmE,QAAQ,CAAC7C,KAAV,EAAiBiD,WAAW,CAAClB,MAA7B,EAAqCE,UAArC,CAA1B;AAEAgB,IAAAA,WAAW,CAAC3B,IAAZ,GAAmBuB,QAAnB;AACD,GArF2D,CAuF5D;;;AACA,QAAMtB,KAAK,CAACgB,OAAN,CAAcrB,GAAd,CAAkBc,IAAI,CAACvC,IAAvB,EAA6B;AACjCC,IAAAA,IAAI,EAAEsC,IAAI,CAACtC,IADsB;AAEjCF,IAAAA,GAAG,EAAEwC,IAAI,CAACxC;AAFuB,GAA7B,CAAN;AAKA,SAAO;AACL+B,IAAAA,KADK;AACEC,IAAAA;AADF,GAAP;AAGD,CAhGD;;AAkGA,MAAMkB,YAAY,GAAIF,QAAD,IAAc;AACjC,MAAIT,MAAM,GAAGS,QAAQ,CAACT,MAAtB;AACA,MAAImB,gBAAgB,GAAGV,QAAQ,CAACQ,GAAhC;AACA,QAAMxB,IAAI,GAAG,CAAC;AACZO,IAAAA,MADY;AAEZF,IAAAA,MAAM,EAAEpD,QAAQ,CAACyE,gBAAD;AAFJ,GAAD,CAAb;AAKAnB,EAAAA,MAAM,GAAGS,QAAQ,CAACT,MAAT,CAAgBoB,OAAzB;AACAD,EAAAA,gBAAgB,GAAGV,QAAQ,CAACT,MAAT,CAAgBqB,YAAnC;;AAEA,SAAOrB,MAAP,EAAe;AACbP,IAAAA,IAAI,CAACuB,IAAL,CAAU;AACRhB,MAAAA,MADQ;AAERF,MAAAA,MAAM,EAAEpD,QAAQ,CAACyE,gBAAD;AAFR,KAAV;AAKAA,IAAAA,gBAAgB,GAAGnB,MAAM,CAACqB,YAA1B;AACArB,IAAAA,MAAM,GAAGA,MAAM,CAACoB,OAAhB;AACD;;AAED3B,EAAAA,IAAI,CAAC6B,OAAL;AAEA,SAAO7B,IAAP;AACD,CAxBD;;AA0BA8B,MAAM,CAACC,OAAP,GAAiBxE,OAAjB","sourcesContent":["'use strict'\n\nconst {\n  DAGLink\n} = require('ipld-dag-pb')\nconst CID = require('cids')\nconst log = require('debug')('ipfs:mfs:core:utils:add-link')\nconst UnixFS = require('ipfs-unixfs')\nconst DirSharded = require('ipfs-unixfs-importer/src/dir-sharded')\nconst {\n  updateHamtDirectory,\n  recreateHamtLevel,\n  createShard,\n  toPrefix,\n  addLinksToHamtBucket\n} = require('./hamt-utils')\nconst errCode = require('err-code')\nconst mc = require('multicodec')\nconst mh = require('multihashes')\nconst last = require('async-iterator-last')\n\nconst addLink = async (context, options) => {\n  if (!options.parentCid && !options.parent) {\n    throw errCode(new Error('No parent node or CID passed to addLink'), 'EINVALIDPARENT')\n  }\n\n  if (options.parentCid && !CID.isCID(options.parentCid)) {\n    throw errCode(new Error('Invalid CID passed to addLink'), 'EINVALIDPARENTCID')\n  }\n\n  if (!options.parent) {\n    log(`Loading parent node ${options.parentCid}`)\n\n    options.parent = await context.ipld.get(options.parentCid)\n  }\n\n  if (!options.cid) {\n    throw errCode(new Error('No child cid passed to addLink'), 'EINVALIDCHILDCID')\n  }\n\n  if (!options.name) {\n    throw errCode(new Error('No child name passed to addLink'), 'EINVALIDCHILDNAME')\n  }\n\n  if (!CID.isCID(options.cid)) {\n    options.cid = new CID(options.cid)\n  }\n\n  if (!options.size && options.size !== 0) {\n    throw errCode(new Error('No child size passed to addLink'), 'EINVALIDCHILDSIZE')\n  }\n\n  const meta = UnixFS.unmarshal(options.parent.Data)\n\n  if (meta.type === 'hamt-sharded-directory') {\n    log('Adding link to sharded directory')\n\n    return addToShardedDirectory(context, options)\n  }\n\n  if (options.parent.Links.length >= options.shardSplitThreshold) {\n    log('Converting directory to sharded directory')\n\n    return convertToShardedDirectory(context, options)\n  }\n\n  log(`Adding ${options.name} (${options.cid}) to regular directory`)\n\n  return addToDirectory(context, options)\n}\n\nconst convertToShardedDirectory = async (context, options) => {\n  const result = await createShard(context, options.parent.Links.map(link => ({\n    name: link.Name,\n    size: link.Tsize,\n    cid: link.Hash\n  })).concat({\n    name: options.name,\n    size: options.size,\n    cid: options.cid\n  }), options)\n\n  log(`Converted directory to sharded directory ${result.cid}`)\n\n  return result\n}\n\nconst addToDirectory = async (context, options) => {\n  options.parent.rmLink(options.name)\n  options.parent.addLink(new DAGLink(options.name, options.size, options.cid))\n\n  const format = mc[options.format.toUpperCase().replace(/-/g, '_')]\n  const hashAlg = mh.names[options.hashAlg]\n\n  // Persist the new parent DAGNode\n  const cid = await context.ipld.put(options.parent, format, {\n    cidVersion: options.cidVersion,\n    hashAlg,\n    hashOnly: !options.flush\n  })\n\n  return {\n    node: options.parent,\n    cid\n  }\n}\n\nconst addToShardedDirectory = async (context, options) => {\n  const {\n    shard, path\n  } = await addFileToShardedDirectory(context, options)\n\n  const result = await last(shard.flush('', context.ipld))\n\n  // we have written out the shard, but only one sub-shard will have been written so replace it in the original shard\n  const oldLink = options.parent.Links\n    .find(link => link.Name.substring(0, 2) === path[0].prefix)\n\n  const newLink = result.node.Links\n    .find(link => link.Name.substring(0, 2) === path[0].prefix)\n\n  if (oldLink) {\n    options.parent.rmLink(oldLink.Name)\n  }\n\n  options.parent.addLink(newLink)\n\n  return updateHamtDirectory(context, options.parent.Links, path[0].bucket, options)\n}\n\nconst addFileToShardedDirectory = async (context, options) => {\n  const file = {\n    name: options.name,\n    cid: options.cid,\n    size: options.size\n  }\n\n  // start at the root bucket and descend, loading nodes as we go\n  const rootBucket = await recreateHamtLevel(options.parent.Links)\n\n  const shard = new DirSharded({\n    root: true,\n    dir: true,\n    parent: null,\n    parentKey: null,\n    path: '',\n    dirty: true,\n    flat: false\n  }, options)\n  shard._bucket = rootBucket\n\n  // load subshards until the bucket & position no longer changes\n  const position = await rootBucket._findNewBucketAndPos(file.name)\n  const path = toBucketPath(position)\n  path[0].node = options.parent\n  let index = 0\n\n  while (index < path.length) {\n    const segment = path[index]\n    index++\n    const node = segment.node\n\n    const link = node.Links\n      .find(link => link.Name.substring(0, 2) === segment.prefix)\n\n    if (!link) {\n      // prefix is new, file will be added to the current bucket\n      log(`Link ${segment.prefix}${file.name} will be added`)\n      index = path.length\n\n      break\n    }\n\n    if (link.Name === `${segment.prefix}${file.name}`) {\n      // file already existed, file will be added to the current bucket\n      log(`Link ${segment.prefix}${file.name} will be replaced`)\n      index = path.length\n\n      break\n    }\n\n    if (link.Name.length > 2) {\n      // another file had the same prefix, will be replaced with a subshard\n      log(`Link ${link.Name} will be replaced with a subshard`)\n      index = path.length\n\n      break\n    }\n\n    // load sub-shard\n    log(`Found subshard ${segment.prefix}`)\n    const subShard = await context.ipld.get(link.Hash)\n\n    // subshard hasn't been loaded, descend to the next level of the HAMT\n    if (!path[index]) {\n      log(`Loaded new subshard ${segment.prefix}`)\n      await recreateHamtLevel(subShard.Links, rootBucket, segment.bucket, parseInt(segment.prefix, 16))\n\n      const position = await rootBucket._findNewBucketAndPos(file.name)\n\n      path.push({\n        bucket: position.bucket,\n        prefix: toPrefix(position.pos),\n        node: subShard\n      })\n\n      break\n    }\n\n    const nextSegment = path[index]\n\n    // add next level's worth of links to bucket\n    await addLinksToHamtBucket(subShard.Links, nextSegment.bucket, rootBucket)\n\n    nextSegment.node = subShard\n  }\n\n  // finally add the new file into the shard\n  await shard._bucket.put(file.name, {\n    size: file.size,\n    cid: file.cid\n  })\n\n  return {\n    shard, path\n  }\n}\n\nconst toBucketPath = (position) => {\n  let bucket = position.bucket\n  let positionInBucket = position.pos\n  const path = [{\n    bucket,\n    prefix: toPrefix(positionInBucket)\n  }]\n\n  bucket = position.bucket._parent\n  positionInBucket = position.bucket._posAtParent\n\n  while (bucket) {\n    path.push({\n      bucket,\n      prefix: toPrefix(positionInBucket)\n    })\n\n    positionInBucket = bucket._posAtParent\n    bucket = bucket._parent\n  }\n\n  path.reverse()\n\n  return path\n}\n\nmodule.exports = addLink\n"]},"metadata":{},"sourceType":"script"}